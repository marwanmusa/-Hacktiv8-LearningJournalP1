# -*- coding: utf-8 -*-
"""KNN_&_Naive_Bayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E9gXcJ9EZ9Gkczz9nH5xv1PwM3wgJ9xl
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from matplotlib import rcParams

rcParams['figure.figsize'] = 15,8

df = pd.read_csv('/content/gene_expression.csv')

df.head()

df.shape

df['Cancer Present'].value_counts()

sns.scatterplot(x='Gene One',y='Gene Two',data=df,hue='Cancer Present',alpha=0.7,style='Cancer Present')

sns.scatterplot(x='Gene One',y='Gene Two',hue='Cancer Present',data=df,style='Cancer Present')
plt.xlim(2,6)
plt.ylim(3,10)
#plt.legend(loc=(1.1,0.5))

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X = df.drop('Cancer Present',axis=1)
y = df['Cancer Present']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=45)

scaler = StandardScaler()

scaler.fit(X_train)

scaled_X_train = scaler.transform(X_train)
scaled_X_test = scaler.transform(X_test)

scaled_X_train

df1 = pd.DataFrame(data=scaled_X_train,columns=X_train.columns,index=X_train.index)

df1

X_train.head()

X_train['Gene One'].plot(kind='hist',bins=50)

df1['Gene One'].plot(kind='hist',bins=50)

"""# KNN"""

from sklearn.neighbors import KNeighborsClassifier

knn_model = KNeighborsClassifier(n_neighbors=5,weights='distance')

knn_model.fit(scaled_X_train,y_train)

"""# MODEL EVALUATION"""

y_pred = knn_model.predict(scaled_X_test)

from sklearn.metrics import classification_report,plot_confusion_matrix,accuracy_score,confusion_matrix,ConfusionMatrixDisplay

accuracy_score(y_test,y_pred)

confusion_matrix(y_test,y_pred)

cm = confusion_matrix(y_test, y_pred, labels=knn_model.classes_)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn_model.classes_)

disp.plot()
plt.show()

print(classification_report(y_test,y_pred))

"""## Elbow Method for Choosing Reasonable K Values

**NOTE: This uses the test set for the hyperparameter selection of K.**
"""

acc = accuracy_score(y_test,y_pred)
err = 1-acc
print(f'hasil error rates {err}')
print(f'hasil accuracy score {acc}')

test_error_rates = []
train_acc_manhattan = []
test_acc_manhattan = []


for k in range(1,30):
    knn_model = KNeighborsClassifier(n_neighbors=k)
    knn_model.fit(scaled_X_train,y_train) 
   
    y_pred_test = knn_model.predict(scaled_X_test)
    
    test_error = 1 - accuracy_score(y_test,y_pred_test)
    test_error_rates.append(test_error)

    train_acc_manhattan.append(knn_model.score(scaled_X_train, y_train))
    test_acc_manhattan.append(knn_model.score(scaled_X_test, y_test))

test_error_rates

plt.figure(figsize=(10,6))
plt.plot(range(1,30),test_error_rates,label='Test Error')
plt.legend()
plt.ylabel('Error Rate')
plt.xlabel("K Value")

plt.figure(figsize=(20,5))
plt.subplot(1, 2, 1)

plt.title('Effect of Value k on Accuracy - Manhattan Distance')
plt.plot(range(1, 30), test_acc_manhattan, label='Testing Accuracy')
plt.plot(range(1, 30), train_acc_manhattan, label='Training accuracy')

plt.legend()
plt.xlabel('Number of k')
plt.ylabel('Accuracy')
plt.show()

"""#NAIVE BAYES"""

from sklearn.naive_bayes import GaussianNB

nb = GaussianNB()

nb.fit(scaled_X_train,y_train)

df_result = X_train.copy()
df_result['prob_no'] =  nb.predict_proba(scaled_X_train)[:,0]
df_result['prob_yes'] =  nb.predict_proba(scaled_X_train)[:,1]
df_result.head()

df_knn = X_train.copy()
df_knn['prob_no'] =  knn_model.predict_proba(scaled_X_train)[:,0]
df_knn['prob_yes'] =  knn_model.predict_proba(scaled_X_train)[:,1]
df_knn.head()

y_pred_nb = nb.predict(scaled_X_test)

accuracy_score(y_test,y_pred_nb)

confusion_matrix(y_test,y_pred_nb)

cm = confusion_matrix(y_test, y_pred_nb, labels=nb.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=nb.classes_)
disp.plot()
plt.show()

print(classification_report(y_test,y_pred_nb))

acc_nb = accuracy_score(y_test,y_pred_nb)
result = acc-acc_nb
print(f'hasil selisih accuracy score KNN dan Naive Bayes adalah {result}')

"""# Trining Time"""

# Commented out IPython magic to ensure Python compatibility.
nb = GaussianNB()
# %timeit nb.fit(scaled_X_train,y_train)

# Commented out IPython magic to ensure Python compatibility.
knn_model = KNeighborsClassifier(n_neighbors=11)
# %timeit knn_model.fit(scaled_X_train,y_train)

# Commented out IPython magic to ensure Python compatibility.
from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
# %timeit logreg.fit(scaled_X_train,y_train)

# Commented out IPython magic to ensure Python compatibility.
# %timeit y_pred_11 = knn_model.predict(scaled_X_test)

# Commented out IPython magic to ensure Python compatibility.
# %timeit y_pred_logreg = logreg.predict(scaled_X_test)

# Commented out IPython magic to ensure Python compatibility.
# %timeit y_pred_nb = nb.predict(scaled_X_test)

accuracy_score(y_test,y_pred_11)

confusion_matrix(y_test,y_pred_11) # accuray n=11

confusion_matrix(y_test,y_pred) # accuracy n=1