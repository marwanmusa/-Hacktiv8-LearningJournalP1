# -*- coding: utf-8 -*-
"""P1W1D2PM - Feature Engineering Part 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GYfjgOGEcX0n2_dlthQ-2QfqyubWg5mZ

# A. Outliers Explanation

## Datasets: 

In this section, we will use the *House Prices* and *Titanic* datasets.
- *House Prices* : from Scikit Learn
- *Titanic* : https://www.openml.org/data/get_csv/16826755/phpMYEkMl
"""

# print information for boston dataset
from sklearn.datasets import load_boston
print(load_boston().DESCR)

"""## Purpose

We will:

- Identify outliers in Normally distributed variables.
- Identify outliers in skewed variables.
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

import scipy.stats as stats

# Boston house dataset for the demo
from sklearn.datasets import load_boston

# Load the Boston House dataset from sklearn
boston_dataset = load_boston()

# Create a dataframe with the independent variables
# Legends : 
  # - CRIM     per capita crime rate by town
  # - RM       average number of rooms per dwelling
  # - LSTAT    % lower status of the population

boston = pd.DataFrame(boston_dataset.data,
                      columns=boston_dataset.feature_names)[[
                          'RM', 'LSTAT', 'CRIM'
                      ]]

print('Shape data : ', boston.shape)
boston.head()

# Load the Titanic dataset

url = 'https://www.openml.org/data/get_csv/16826755/phpMYEkMl'
titanic = pd.read_csv(url, usecols=['age', 'fare'])

# The variables age and fare have missing values. For this missing value, let's remove them.
titanic.dropna(subset=['age', 'fare'], inplace=True)
titanic['age'] = pd.to_numeric(titanic['age'], errors = 'coerce')
titanic['fare'] = pd.to_numeric(titanic['fare'], errors = 'coerce')
titanic['age'] = titanic['age'].astype(float)
titanic['fare'] = titanic['fare'].astype(float)

print('Shape data : ', titanic.shape, '\n')
print(titanic.info(), '\n')
titanic.head()

"""## Identify variable distribution

In Normally distributed variables, outliers are those values that lie beyond the mean plus or minus 3 times the standard deviation. If the variables are skewed however, we find outliers using the inter-quantile range. In order to decide which method to utilise to detect outliers, we first need to know the distribution of the variable.

We can use histograms to determine if the variable is normally distributed. We can also use boxplots to directly visualise the outliers. Boxplots are a standard way of displaying the distribution of a variable utilising the first quartile, the median, the third quartile and the whiskers.

Looking at a boxplot, you can easily identify:

- The median, indicated by the line within the box.
- The inter-quantile range (IQR), the box itself.
- The quantiles, 25th (Q1) is the lower and 75th (Q3) the upper end of the box.
- The wiskers, which extend to: 
  -- top whisker: Q3 + 1.5 x IQR
  -- bottom whisker: Q1 -1.5 x IQR

Any value sitting outside the whiskers is considered an outlier. Let's look at the examples below.
"""

# Function to create histogram and boxplot.
# This functions takes a dataframe (df) and the variable of interest as arguments.

def diagnostic_plots(df, variable):
    # Define figure size
    plt.figure(figsize=(16, 4))

    # Histogram
    plt.subplot(1, 2, 1)
    sns.histplot(df[variable], bins=30)
    plt.title('Histogram')

    # Boxplot
    plt.subplot(1, 2, 2)
    sns.boxplot(y=df[variable])
    plt.title('Boxplot')

    plt.show()

"""### Normally distributed variables"""

# Let's start with the variable RM from the Boston House dataset.
# RM is the average number of rooms per dwelling

diagnostic_plots(boston, 'RM')
print('\nSkewness Value : ', boston['RM'].skew())

"""From the histogram, we see that the variable rm approximates a Gaussian distribution quite well. In the boxplot, we see that the variable could have outliers, as there are many dots sitting outside the whiskers, at both tails of the distribution."""

# let's inspect now the variable Age from the titanic
# refers to the age of the passengers on board

diagnostic_plots(titanic, 'age')
print('\nSkewness Value : ', titanic['age'].skew())

"""From the histogram, we see that the variable approximates fairly well a Gaussian distribution. There is a deviation from the distribution towards the smaller values of age. In the boxplot, we can see that the variable could have outliers, as there are many dots sitting outside the whiskers, at the right end of the distribution (top whisker in the boxplot).

### Skewed variables
"""

# Variable LSTAT from the boston house dataset
# LSTAT is the % lower status of the population

diagnostic_plots(boston, 'LSTAT')
print('\nSkewness Value : ', boston['LSTAT'].skew())

"""LSTAT is not normally distributed, it is skewed with a tail to the right. According to the boxplot, there are some outliers at the right end of the distribution of the variable."""

# Variable CRIM from the boston house dataset
# CRIM is the per capita crime rate by town

diagnostic_plots(boston, 'CRIM')
print('\nSkewness Value : ', boston['CRIM'].skew())

"""CRIM is heavily skewed, with a tail to the right. There seems to be quite a few outliers as well at the right end of the distribution, according to the boxplot."""

# Variable Fare from the titanic dataset
# Fare is the price paid for the ticket by the passengers

diagnostic_plots(titanic, 'fare')
print('\nSkewness Value : ', titanic['fare'].skew())

"""Fare is also very skewed, and shows some unusual values at the right end of its distribution.

In the next cells, we will identify outliers using :
- the mean and the standard deviation for the variables `RM` and `Age` from the boston and titanic datasets, respectively. 
- the inter-quantile range to identify outliers for the variables `LSTAT`, `CRIM` and `Fare` from the boston and titanic datasets.

### Outlier detection for Normally distributed variables
"""

# Function to find upper and lower boundaries for normally distributed variables
# Calculate the boundaries outside which sit the outliers for a Gaussian distribution

def find_normal_boundaries(df, variable):
    upper_boundary = df[variable].mean() + 3 * df[variable].std()
    lower_boundary = df[variable].mean() - 3 * df[variable].std()

    return upper_boundary, lower_boundary

# calculate boundaries for RM
upper_boundary, lower_boundary = find_normal_boundaries(boston, 'RM')
upper_boundary, lower_boundary

"""From the above we conclude that values **bigger than 8.4** or **smaller than 4.2** occur very rarely for the variable `RM`. Therefore, we can consider them outliers."""

# Inspect the number and percentage of outliers for `RM`

print('Total number of houses: {}'.format(len(boston)))
print('Houses with more than 8.4 rooms (right end outliers) : {}'.format(len(boston[boston['RM'] > upper_boundary])))
print('Houses with less than 4.2 rooms (left end outliers.  : {}'.format(len(boston[boston['RM'] < lower_boundary])))
print('')
print('% right end outliers : {}'.format(len(boston[boston['RM'] > upper_boundary]) / len(boston) * 100))
print('% left end outliers  : {}'.format(len(boston[boston['RM'] < lower_boundary]) / len(boston) * 100))

"""Using Extreme Value Analysis we identified outliers at both ends of the distribution of `RM`. The percentage of outliers is small (1.58% considering the 2 tails together), which makes sense, because we are finding precisely outliers. That is, rare values, rare occurrences.

Let's move on to `Age` in the titanic dataset.
"""

# Calculate boundaries for `Age` in the titanic

upper_boundary, lower_boundary = find_normal_boundaries(titanic, 'age')
upper_boundary, lower_boundary

"""The upper boundary is **73 years**, which means that passengers older than 73 were very few, if any, in the titanic. The lower boundary is negative. Because negative age does not exist, it only makes sense to look for outliers utilising the upper boundary."""

# Lets look at the number and percentage of outliers

print('Total passengers         : {}'.format(len(titanic)))
print('Passengers older than 73 : {}'.format(len(titanic[titanic['age'] > upper_boundary])))
print('')
print('% of passengers older than 73 : {}'.format(len(titanic[titanic['age'] > upper_boundary]) / len(titanic) * 100))

"""There were 3 passengers older than 73 on board of the titanic, which could be considered outliers, as the majority of the population where much younger.

### Outlier detection for skewed variables
"""

# Function to find upper and lower boundaries for skewed distributed variables
# Let's calculate the boundaries outside which sit the outliers for skewed distributions
# Distance passed as an argument, gives us the option to estimate 1.5 times or 3 times the IQR to calculate the boundaries.

def find_skewed_boundaries(df, variable, distance):
    IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)

    lower_boundary = df[variable].quantile(0.25) - (IQR * distance)
    upper_boundary = df[variable].quantile(0.75) + (IQR * distance)

    return upper_boundary, lower_boundary

# Looking for outliers, using the interquantile proximity rule IQR * 1.5, for `LSTAT` in the boston house dataset

upper_boundary, lower_boundary = find_skewed_boundaries(boston, 'LSTAT', 1.5)
upper_boundary, lower_boundary

# Lets look at the number and percentage of outliers for `LSTAT`

print('Total houses : {}'.format(len(boston)))
print('Houses with LSTAT bigger than 32   : {}'.format(len(boston[boston['LSTAT'] > upper_boundary])))
print('')
print('% houses with LSTAT bigger than 32 : {}'.format(len(boston[boston['LSTAT'] > upper_boundary])/len(boston) * 100))

"""The upper boundary shows a value of ~32. The lower boundary is negative, however the variable `LSTAT` does not take negative values. So to calculate the outliers for LSTAT we only use the upper boundary. This coincides with what we observed in the boxplot earlier in the notebook. Outliers sit only at the right tail of LSTAT's distribution. **We observe 7 houses, 1.38 % of the dataset, with extremely high values for `LSTAT`.**"""

# Looking for outliers, using the interquantile proximity rule IQR * 1.5, for `CRIM` in the boston house dataset

upper_boundary, lower_boundary = find_skewed_boundaries(boston, 'CRIM', 1.5)
upper_boundary, lower_boundary

# Lets look at the number and percentage of outliers for `CRIM`

print('Total houses : {}'.format(len(boston)))
print('Houses with CRIM bigger than 9   : {}'.format(len(boston[boston['CRIM'] > upper_boundary])))
print('')
print('% houses with CRIM bigger than 9 : {}'.format(len(boston[boston['CRIM'] > upper_boundary]) / len(boston) * 100))

# Looking for outliers, using the interquantile proximity rule IQR * 3, now lets looking for extremely high values for `CRIM` in the boston house dataset

upper_boundary, lower_boundary = find_skewed_boundaries(boston, 'CRIM', 3)
upper_boundary, lower_boundary

# Lets look at the number and percentage of outliers for `CRIM`

print('Total houses : {}'.format(len(boston)))
print('Houses with CRIM bigger than 14   : {}'.format(len(boston[boston['CRIM'] > upper_boundary])))
print('')
print('% houses with CRIM bigger than 14 : {}'.format(len(boston[boston['CRIM'] > upper_boundary]) / len(boston) * 100))

"""When using :
- the 1.5 times inter-quantile range to find outliers, we find that **~13%** of the houses show unusually high crime rate areas. 
- the 3 times inter-quantile range to find outliers, we find that **~6%** of the houses show unusually high crime rate areas. 

For `CRIM` as well, the lower boundary is negative, so it only makes sense to use the upper boundary to calculate outliers, as the variable takes only positive values. This coincides with what we observed in `CRIM`'s boxplot earlier in this notebook.
"""

# Finally, identify outliers in `Fare` in the titanic dataset. Lets look again for extreme values using IQR * 3

upper_boundary, lower_boundary = find_skewed_boundaries(titanic, 'fare', 3)
upper_boundary, lower_boundary

# Lets look at the number and percentage of passengers who paid extremely high Fares

print('Total passengers : {}'.format(len(titanic)))
print('Passengers who paid more than 117   : {}'.format(len(titanic[titanic['fare'] > upper_boundary])))
print('')
print('% passengers who paid more than 117 : {}'.format(len(titanic[titanic['fare'] > upper_boundary])/len(titanic) * 100))

"""For Fare, as well as for all the other variables in this notebook which show a tail to the right, the lower boundary is negative. So we will use the upper boundary to determine the outliers. We observe that 6.4% of the values of the dataset fall above the boundary.

# B. Trimming

## Trimming or truncation

Trimming, also known as truncation, involves removing the outliers from the dataset. We only need to decide on a metric to determine outliers. As we saw in section 3, this can be the Gaussian approximation for normally distributed variables or the inter-quantile range proximity rule for skewed variables.

### Advantages

- quick

### Limitations

- outliers for one variable could contain useful information in the other variables
- if there are outliers across many variables, we could remove a big chunk of dataset

## Important

Outliers should be detected AND **removed ONLY** from the training set, and NOT from the test set. So we should first divide our data set into train and tests, and remove outliers in the train set, but keep those in the test set, and measure how well our model is doing.

## Purpose

We will see how to perform **trimming** using the Boston House Dataset.
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# Boston house dataset for the demo
from sklearn.datasets import load_boston

# Load the Boston House dataset from sklearn
boston_dataset = load_boston()

# Create a dataframe with the independent variables
# Legends : 
  # - DIS      weighted distances to five Boston employment centres
  # - RM       average number of rooms per dwelling
  # - LSTAT    % lower status of the population
  
boston = pd.DataFrame(boston_dataset.data,
                      columns=boston_dataset.feature_names)[[
                          'DIS', 'LSTAT', 'CRIM'
                      ]]

print('Shape data : ', boston.shape)
boston.head(10)

# Function to create histogram and boxplot.
# Function takes a dataframe (df) and the variable of interest as arguments

def diagnostic_plots(df, variable):
    # Define figure size
    plt.figure(figsize=(16, 4))

    # Histogram
    plt.subplot(1, 2, 1)
    sns.histplot(df[variable], bins=30)
    plt.title('Histogram')

    # Boxplot
    plt.subplot(1, 2, 2)
    sns.boxplot(y=df[variable])
    plt.title('Boxplot')

    plt.show()

# Let's find outliers in `DIS`, `LSTAT`, `CRIM`

diagnostic_plots(boston, 'DIS')
diagnostic_plots(boston, 'LSTAT')
diagnostic_plots(boston, 'CRIM')

"""There are outliers in all of the above variables."""

# Function to find upper and lower boundaries for skewed distributed variables
# Let's calculate the boundaries outside which sit the outliers for skewed distributions
# Distance passed as an argument, gives us the option to estimate 1.5 times or 3 times the IQR to calculate the boundaries.

def find_skewed_boundaries(df, variable, distance):
    IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)

    lower_boundary = df[variable].quantile(0.25) - (IQR * distance)
    upper_boundary = df[variable].quantile(0.75) + (IQR * distance)

    return upper_boundary, lower_boundary

# Limits for `DIS`
DIS_upper_limit, DIS_lower_limit = find_skewed_boundaries(boston, 'DIS', 1.5)
DIS_upper_limit, DIS_lower_limit

# Limits for `LSTAT`
LSTAT_upper_limit, LSTAT_lower_limit = find_skewed_boundaries(boston, 'LSTAT', 1.5)
LSTAT_upper_limit, LSTAT_lower_limit

# Limits for `CRIM`
CRIM_upper_limit, CRIM_lower_limit = find_skewed_boundaries(boston, 'CRIM', 1.5)
CRIM_upper_limit, CRIM_lower_limit

print('DIS_upper_limit : ', DIS_upper_limit)
print('DIS_lower_limit : ', DIS_lower_limit, '\n')
print('LSTAT_upper_limit : ', LSTAT_upper_limit)
print('LSTAT_lower_limit : ', LSTAT_lower_limit, '\n')
print('CRIM_upper_limit : ', CRIM_upper_limit)
print('CRIM_lower_limit : ', CRIM_lower_limit)

# Flag the outliers in category `DIS`
outliers_DIS = np.where(boston['DIS'] > DIS_upper_limit, True,
                       np.where(boston['DIS'] < DIS_lower_limit, True, False))

# Flag the outliers in category `LSTAT`
outliers_LSTAT = np.where(boston['LSTAT'] > LSTAT_upper_limit, True,
                       np.where(boston['LSTAT'] < LSTAT_lower_limit, True, False))

# Flag the outliers in category `CRIM`
outliers_CRIM = np.where(boston['CRIM'] > CRIM_upper_limit, True,
                       np.where(boston['CRIM'] < CRIM_lower_limit, True, False))

print(outliers_DIS[:10])

# Let's trimm the dataset

boston_trimmed = boston.loc[~(outliers_DIS + outliers_LSTAT + outliers_CRIM)]
print('Size dataset - Before trimming : ', boston.shape)
print('Size dataset - After trimming  : ', boston_trimmed.shape)

"""We can see that using trimming, **we removed 72 rows**, from a dataset of 506 rows, this is about **~14.2%** of the data was removed. This is mostly why, we do not tend to use trimming much in machine learning. But if only a few variables present a tiny proportion of outliers, trimming could work."""

# Let's find outliers in `DIS`, `LSTAT`, `CRIM`

print('DIS - Before Trimming')
diagnostic_plots(boston, 'DIS')
print('\nDIS - After Trimming')
diagnostic_plots(boston_trimmed, 'DIS')

print('\nLSTAT - Before Trimming')
diagnostic_plots(boston, 'LSTAT')
print('\nLSTAT - After Trimming')
diagnostic_plots(boston_trimmed, 'LSTAT')

print('\nCRIM - Before Trimming')
diagnostic_plots(boston, 'CRIM')
print('\nCRIM - After Trimming')
diagnostic_plots(boston_trimmed, 'CRIM')

"""For `LSTAT` and `CRIM`, we still see many outliers. **When we remove data points from our dataset, all the parameters of the distribution are re-calculated**, those are the mean, quantiles and inter-quantile range, therefore, in the new -trimmed- variable, values that before were not considered outliers, now are. This is an unwanted characteristic of this way of coping with outliers.

# C. Censoring

## Censoring or Capping.

**Censoring**, or **capping**, means capping the maximum and /or minimum of a distribution at an arbitrary value. On other words, values bigger or smaller than the arbitrarily determined ones are **censored**.

Capping can be done at both tails, or just one of the tails, depending on the variable and the user.

### Advantages

- does not remove data

### Limitations

- distorts the distributions of the variables
- distorts the relationships among variables

## Important

When doing capping, we tend to cap values both in train and test set. It is important to remember that the capping values MUST be derived from the train set. And then use those same values to cap the variables in the test set

## Purpose

We will see how to perform **capping** with the inter-quantile range bold textproximity rule using the Boston House Dataset
"""

!pip install feature-engine

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# Boston house dataset for the demo
from sklearn.datasets import load_boston

from feature_engine.outliers import Winsorizer

# Load the Boston House dataset from sklearn
boston_dataset = load_boston()

# Create a dataframe with the independent variables
# Legends : 
  # - CRIM     per capita crime rate by town
  # - DIS      weighted distances to five Boston employment centres
  # - LSTAT    % lower status of the population
  # - MEDV     Median value of owner-occupied homes in $1000's

boston = pd.DataFrame(boston_dataset.data,
                      columns=boston_dataset.feature_names)[[
                          'DIS', 'LSTAT', 'CRIM'
                      ]]

# Add the target
boston['MEDV'] = boston_dataset.target

print('Shape data : ', boston.shape)
boston.head()

# Create the capper

windsoriser = Winsorizer(capping_method='iqr', # choose iqr for IQR rule boundaries or gaussian for mean and std
                          tail='both', # cap left, right or both tails 
                          fold=1.5,
                          variables=['DIS', 'LSTAT', 'CRIM'])

windsoriser.fit(boston)

boston_t = windsoriser.transform(boston)

# Inspect the minimum caps for each variable
print('Left Tail Caps  : ', windsoriser.left_tail_caps_)

# Iinspect the maximum caps for each variable
print('Right Tail Caps : ', windsoriser.right_tail_caps_)

print('Boston Dataframe - Before Capping')
print(boston.describe())
print('')
print('Boston Dataframe - After Capping')
print(boston_t.describe())

# Let's find outliers in `DIS`, `LSTAT`, `CRIM`

print('DIS - Before Capping')
diagnostic_plots(boston, 'DIS')
print('\nDIS - After Capping')
diagnostic_plots(boston_t, 'DIS')

print('\nLSTAT - Before Capping')
diagnostic_plots(boston, 'LSTAT')
print('\nLSTAT - After Capping')
diagnostic_plots(boston_t, 'LSTAT')

print('\nCRIM - Before Capping')
diagnostic_plots(boston, 'CRIM')
print('\nCRIM - After Capping')
diagnostic_plots(boston_t, 'CRIM')

"""# E. Cardinality

The values of a categorical variable are selected from a group of categories, also called labels. For example, in the variable _gender_ the categories or labels are male and female, whereas in the variable _city_ the labels can be London, Manchester, Brighton and so on.

Different categorical variables contain different number of labels or categories. The variable gender contains only 2 labels, but a variable like city or postcode, can contain a huge number of different labels.

The number of different labels within a categorical variable is known as cardinality. A high number of labels within a variable is known as __high cardinality__.


### Are multiple labels in a categorical variable a problem?

High cardinality may pose the following problems: 

- Variables with too many labels **tend to dominate** over those with only a few labels, particularly in **Tree based** algorithms.

- A big number of labels within a variable may introduce noise with little, if any, information, therefore making machine learning models **prone to over-fit**.

- Some of the labels may **only be present in the training data set, but not in the test set**, therefore machine learning algorithms may over-fit to the training set.

- Contrarily, **some labels may appear only in the test set**, therefore leaving the machine learning algorithms unable to perform a calculation over the new (unseen) observation.


In particular, **tree methods can be biased towards variables with lots of labels** (variables with high cardinality). Thus, their performance may be affected by high cardinality.

## Purpose :

We will :

- Learn how to quantify cardinality
- See examples of high and low cardinality variables
- Understand the effect of cardinality when preparing train and test sets
- Visualise the effect of cardinality on Machine Learning Model performance

We will use the Titanic dataset. *(URL : https://www.openml.org/data/get_csv/16826755/phpMYEkMl)*
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier

from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split

# Load the Titanic dataset

url = 'https://www.openml.org/data/get_csv/16826755/phpMYEkMl'
data = pd.read_csv(url)
data.head()

data.info()

"""Let's focus on several categorical variables in this dataset : `Name`, `Sex`, `Ticket`, `Cabin`, and `Embarked`."""

# Let's inspect the cardinality, this is the number of different labels, for the different categorical variables

print('Number of categories in the variable Name     : {}'.format(len(data.name.unique())))
print('Number of categories in the variable Gender   : {}'.format(len(data.sex.unique())))
print('Number of categories in the variable Ticket   : {}'.format(len(data.ticket.unique())))
print('Number of categories in the variable Cabin    : {}'.format(len(data.cabin.unique())))
print('Number of categories in the variable Embarked : {}'.format(len(data.embarked.unique())))
print('Total number of passengers in the Titanic.    : {}'.format(len(data)))

"""While the variable `Sex` contains only 2 categories and `Embarked` 4 (low cardinality), the variables `Ticket`, `Name` and `Cabin`, as expected, contain a huge number of different labels (high cardinality).

To demonstrate the effect of high cardinality in train and test sets and machine learning performance, let's work with the variable `Cabin`. We will create a new variable to reduced cardinality.
"""

# Let's explore the values / categories of `Cabin`
# We know from the previous cell that there are 187 different cabins, therefore the variable is highly cardinal
data.cabin.unique()

"""Let's now reduce the cardinality of the variable. How? Instead of using the entire `cabin` value, we will capture only the 
**first letter**.

***Rationale***: the first letter indicates the deck on which the cabin was located, and is therefore an indication of both social class status and proximity to the surface of the Titanic. Both are known to improve the probability of survival.
"""

# Let's capture the first letter of Cabin

data['Cabin_reduced'] = data['cabin'].astype(str).str[0]

print(data[['cabin', 'Cabin_reduced']].head())
print('Number of categories in the variable Cabin : {}'.format(len(data.cabin.unique())))
print('Number of categories in the variable Cabin reduced : {}'.format(len(data.Cabin_reduced.unique())))

"""We reduced the number of different labels from 187 to 9."""

# Let's separate into training and testing set in order to build machine learning models

use_cols = ['cabin', 'Cabin_reduced', 'sex']

X_train, X_test, y_train, y_test = train_test_split(data[use_cols], 
                                                    data['survived'],  
                                                    test_size=0.3,
                                                    random_state=0)

X_train.shape, X_test.shape

"""### High cardinality leads to uneven distribution of categories in train and test sets

When a variable is highly cardinal, often some categories land only on the training set, or only on the testing set. If present only in the training set, they may lead to over-fitting. If present only on the testing set, the machine learning algorithm will not know how to handle them, as it has not seen them during training.
"""

# Let's find out labels that only present in the training set and labels that only present in the test set

unique_to_train_set = [x for x in X_train.cabin.unique() if x not in X_test.cabin.unique()]
unique_to_test_set = [x for x in X_test.cabin.unique() if x not in X_train.cabin.unique()]

print('Total unique_to_train_set : ', len(unique_to_train_set))
print('Total unique_to_test_set  : ', len(unique_to_test_set))

"""There are **116 Cabins only present in the training set**, and not in the testing set. Also, there are **36 Cabins that only present in the test set** but not in the train set.

Variables with high cardinality tend to have values (i.e., categories) present in the training set, that are not present in the test set, and vice versa. This will bring problems at the time of training (due to over-fitting) and scoring of new data (how should the model deal with unseen categories?). This problem is almost overcome by reducing the cardinality of the variable.
"""

# Let's find out labels that only present in the training set and only present in the test set for Cabin with reduced cardinality

unique_to_train_set = [
    x for x in X_train['Cabin_reduced'].unique()
    if x not in X_test['Cabin_reduced'].unique()
]

unique_to_test_set = [
    x for x in X_test['Cabin_reduced'].unique()
    if x not in X_train['Cabin_reduced'].unique()
]

print('Total unique_to_train_set : ', len(unique_to_train_set))
print('Total unique_to_test_set  : ', len(unique_to_test_set))

"""Observe how by reducing the cardinality there is now **only 1 label in the training set** that is not present in the test set. And **no label in the test set** that is not contained in the training set as well.

### Effect of cardinality on Machine Learning Model Performance

In order to evaluate the effect of categorical variables in machine learning models, we will quickly replace the categories by numbers. See below.
"""

# Let's re-map Cabin into numbers so we can use it to train ML models

# I will replace each cabin by a number
# to quickly demonstrate the effect of
# labels on machine learning algorithms

cabin_dict = {k: i for i, k in enumerate(X_train.cabin.unique(), 0)}
print('cabin_dict : ', cabin_dict)
print('Total Cabin Unique - X_train : ', len(X_train.cabin.unique()))
print('Total Cabin Unique - Data    : ', len(data.cabin.unique()))

# Replace the labels in Cabin, using the dict `cabin_dict` created above
X_train.loc[:, 'Cabin_mapped'] = X_train.loc[:, 'cabin'].map(cabin_dict)
X_test.loc[:, 'Cabin_mapped'] = X_test.loc[:, 'cabin'].map(cabin_dict)

X_train[['Cabin_mapped', 'cabin']].head(10)

# Display First 10 Data in `X_train`
X_train.head(10)

"""We see how NaN takes the value 0 in the new variable, E36 takes the value 1, C68 takes the value 2, and so on."""

# Now we will replace the letters in the reduced cabin variable (`Cabin_reduced`) with the same procedure

# Create replace dictionary
cabin_dict = {k: i for i, k in enumerate(X_train['Cabin_reduced'].unique(), 0)}

print('Before Replace')
print(X_train[['Cabin_reduced', 'cabin']].head(10))
print('')

# Replace labels by numbers with dictionary
X_train.loc[:, 'Cabin_reduced'] = X_train.loc[:, 'Cabin_reduced'].map(cabin_dict)
X_test.loc[:, 'Cabin_reduced'] = X_test.loc[:, 'Cabin_reduced'].map(cabin_dict)

print('After Replace')
print(X_train[['Cabin_reduced', 'cabin']].head(10))
print('')

# Display First 10 Data in `X_train`
X_train.head(10)

"""We see now that E36 and E24 take the same number, 1, because we are capturing only the letter. They both start with E."""

# Re-map the categorical variable `Sex` into numbers

X_train.loc[:, 'sex'] = X_train.loc[:, 'sex'].map({'male': 0, 'female': 1})
X_test.loc[:, 'sex'] = X_test.loc[:, 'sex'].map({'male': 0, 'female': 1})

X_train.sex.head()

# Check if there are missing values in these variables

X_train[['Cabin_mapped', 'Cabin_reduced', 'sex']].isnull().sum()

X_test[['Cabin_mapped', 'Cabin_reduced', 'sex']].isnull().sum()

# Display row in `X_test` that contains missing values
X_test[X_test.isna().any(axis=1)]

"""In the test set, there are now **42 missing values** for the highly cardinal variable. These were introduced when encoding the categories into numbers. How? **Many categories exist only in the test set.** Thus, when we created our encoding dictionary using only the train set, we did not generate a number to replace those labels present only in the test set. As a consequence, they were encoded as NaN. For now, let's fill those missing values with 0."""

# Display X_train
X_train.head(10)

"""### Random Forests"""

# Model built on data with high cardinality for cabin

# Call the model
rf = RandomForestClassifier(n_estimators=200, random_state=39)

# Train the model
rf.fit(X_train[['Cabin_mapped', 'sex']], y_train)

# Make predictions on train and test set
pred_train = rf.predict_proba(X_train[['Cabin_mapped', 'sex']])
pred_test = rf.predict_proba(X_test[['Cabin_mapped', 'sex']].fillna(0))

print('Train set')
print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred_train[:,1])))
print('Test set')
print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred_test[:,1])))

"""We observe that the performance of the Random Forests on the training set is quite superior to its performance in the test set. This indicates that the model is over-fitting, which means that it does a great job at predicting the outcome on the dataset it was trained on, but it lacks the power to generalise the prediction to unseen data."""

# Model built on data with low cardinality for cabin

# Call the model
rf = RandomForestClassifier(n_estimators=200, random_state=39)

# Train the model
rf.fit(X_train[['Cabin_reduced', 'sex']], y_train)

# Make predictions on train and test set
pred_train = rf.predict_proba(X_train[['Cabin_reduced', 'sex']])
pred_test = rf.predict_proba(X_test[['Cabin_reduced', 'sex']])

print('Train set')
print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred_train[:,1])))
print('Test set')
print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred_test[:,1])))

"""We can see now that the Random Forests no longer over-fit to the training set. In addition, the model is much better at generalising the predictions (compare the roc-auc of this model on the test set vs the roc-auc of the model above also in the test set: 0.81 vs 0.80).

**Given a same model, with identical hyper-parameters, high cardinality may cause the model to over-fit**.

### Logistic Regression
"""

# Model build on data with plenty of categories in Cabin variable

# Call the model
logit = LogisticRegression(random_state=44, solver='lbfgs')

# Train the model
logit.fit(X_train[['Cabin_mapped', 'sex']], y_train)

# Make predictions on train and test set
pred_train = logit.predict_proba(X_train[['Cabin_mapped', 'sex']])
pred_test = logit.predict_proba(X_test[['Cabin_mapped', 'sex']].fillna(0))

print('Train set')
print('Logistic regression roc-auc: {}'.format(roc_auc_score(y_train, pred_train[:,1])))
print('Test set')
print('Logistic regression roc-auc: {}'.format(roc_auc_score(y_test, pred_test[:,1])))

# Model build on data with fewer categories in Cabin Variable

# Call the model
logit = LogisticRegression(random_state=44, solver='lbfgs')

# Train the model
logit.fit(X_train[['Cabin_reduced', 'sex']], y_train)

# Make predictions on train and test set
pred_train = logit.predict_proba(X_train[['Cabin_reduced', 'sex']])
pred_test = logit.predict_proba(X_test[['Cabin_reduced', 'sex']].fillna(0))

print('Train set')
print('Logistic regression roc-auc: {}'.format(roc_auc_score(y_train, pred_train[:,1])))
print('Test set')
print('Logistic regression roc-auc: {}'.format(roc_auc_score(y_test, pred_test[:,1])))

"""We can draw the same conclusion for Logistic Regression : **reducing the cardinality improves the performance and generalisation of the algorithm**."""