# -*- coding: utf-8 -*-
"""Decision_Trees_&_Ensamble_Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xlgGqC_3QfcB21iibu6bAVD-ydKw4XCu

# Decision Trees

## Imports
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.simplefilter("ignore")

df = pd.read_csv("/content/penguins_size.csv")

df.head()

"""## EDA

### Missing Data

Recall the purpose is to create a model for future use, so data points missing crucial information won't help in this task, especially since for future data points we will assume the research will grab the relevant feature information.
"""

df.info()

df.isna().sum()

# What percentage are we dropping?
100*(10/344)

df = df.dropna()

df.info()

df.head()

df['sex'].unique()

df['island'].unique()

df = df[df['sex']!='.']

"""## Visualization"""

sns.scatterplot(x='culmen_length_mm',y='culmen_depth_mm',data=df,hue='species',palette='Dark2')

sns.pairplot(df,hue='species',palette='Dark2')

sns.catplot(x='species',y='culmen_length_mm',data=df,kind='box',col='sex',palette='Dark2')

"""## Feature Engineering"""

pd.get_dummies(df)

pd.get_dummies(df.drop('species',axis=1),drop_first=True)

"""## Train | Test Split"""

X = pd.get_dummies(df.drop('species',axis=1),drop_first=True)
y = df['species']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)

"""# Decision Tree Classifier

## Default Hyperparameters
"""

from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier()

model.fit(X_train,y_train)

base_pred = model.predict(X_test)
base_pred_train = model.predict(X_train)

"""## Evaluation"""

from sklearn.metrics import confusion_matrix,classification_report,plot_confusion_matrix,accuracy_score

plot_confusion_matrix(model,X_test,y_test)

accuracy_score(y_test,base_pred)

accuracy_score(y_train,base_pred_train)

model.feature_importances_

pd.DataFrame(index=X.columns,data=model.feature_importances_,columns=['Feature Importance'])

"""## Visualize the Tree

This function is fairly new, you may want to review the online docs:

Online Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html
"""

from sklearn.tree import plot_tree

plt.figure(figsize=(12,8),dpi=150)
plot_tree(model,filled=True,feature_names=X.columns);

"""##Pruning"""

X = pd.get_dummies(df.drop(['species','body_mass_g'],axis=1),drop_first=True)
y = df['species']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)

path=model.cost_complexity_pruning_path(X_train,y_train)
alphas = path['ccp_alphas']
alphas

from sklearn.metrics import accuracy_score
accuracy_train,accuracy_test=[],[]

for i in alphas :
  tree = DecisionTreeClassifier(criterion="gini",ccp_alpha=i)

  tree.fit(X_train,y_train)
  y_train_pred = tree.predict(X_train)
  y_test_pred = tree.predict(X_test)

  accuracy_train.append(accuracy_score(y_train,y_train_pred))
  accuracy_test.append(accuracy_score(y_test,y_test_pred))


sns.set()
plt.figure(figsize=(14,7))
sns.lineplot(y=accuracy_train,x=alphas, label='Train Accuracy')
sns.lineplot(y=accuracy_test,x=alphas, label='Test Accuracy')
#plt.xticks(ticks=np.arrange(0.00,0.25,0.01))
plt.show()

"""##Pruning from Doct Sklearn"""

clf = DecisionTreeClassifier(random_state=0)
path = clf.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities

fig, ax = plt.subplots()
ax.plot(ccp_alphas[:-1], impurities[:-1], marker="o", drawstyle="steps-post")
ax.set_xlabel("effective alpha")
ax.set_ylabel("total impurity of leaves")
ax.set_title("Total Impurity vs effective alpha for training set")

clfs = []
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)
    clf.fit(X_train, y_train)
    clfs.append(clf)
print(
    "Number of nodes in the last tree is: {} with ccp_alpha: {}".format(
        clfs[-1].tree_.node_count, ccp_alphas[-1]
    )
)

clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]

node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
fig, ax = plt.subplots(2, 1)
ax[0].plot(ccp_alphas, node_counts, marker="o", drawstyle="steps-post")
ax[0].set_xlabel("alpha")
ax[0].set_ylabel("number of nodes")
ax[0].set_title("Number of nodes vs alpha")
ax[1].plot(ccp_alphas, depth, marker="o", drawstyle="steps-post")
ax[1].set_xlabel("alpha")
ax[1].set_ylabel("depth of tree")
ax[1].set_title("Depth vs alpha")
fig.tight_layout()

train_scores = [clf.score(X_train, y_train) for clf in clfs]
test_scores = [clf.score(X_test, y_test) for clf in clfs]

fig, ax = plt.subplots()
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy vs alpha for training and testing sets")
ax.plot(ccp_alphas, train_scores, marker="o", label="train", drawstyle="steps-post")
ax.plot(ccp_alphas, test_scores, marker="o", label="test", drawstyle="steps-post")
ax.legend()
plt.show()

"""# DecisionTree Regression"""

import pandas as pd
import numpy as np
from sklearn import datasets

boston = datasets.load_boston()

df = pd.DataFrame(boston.data, columns = boston.feature_names)
df['target'] = boston.target
df.sample(5)

#Lets create feature matrix X  and y labels
X_boston = df.drop('target', axis=1)
y_boston = df['target']

X_train_boston,X_test_boston, y_train_boston, y_test_boston = train_test_split(X_boston, y_boston, test_size= 0.2, random_state= 1)

from sklearn.tree import DecisionTreeRegressor
dtr = DecisionTreeRegressor()
dtr.fit(X_train_boston ,y_train_boston)

from sklearn.metrics import mean_squared_error 
y_pred_boston = dtr.predict(X_test_boston)
np.sqrt(mean_squared_error(y_test_boston, y_pred_boston))

y_pred_train_boston = dtr.predict(X_train_boston)
np.sqrt(mean_squared_error(y_train_boston, y_pred_train_boston))

path=dtr.cost_complexity_pruning_path(X_train_boston,y_train_boston)
alphas = path['ccp_alphas']

rmse_train,rmse_test=[],[]

for i in alphas :
  tree = DecisionTreeRegressor(random_state= 1,ccp_alpha=i)

  tree.fit(X_train_boston,y_train_boston)
  y_train_pred_boston = tree.predict(X_train_boston)
  y_test_pred_boston = tree.predict(X_test_boston)

  rmse_train.append(np.sqrt(mean_squared_error(y_train_boston,y_train_pred_boston)))
  rmse_test.append(np.sqrt(mean_squared_error(y_test_boston,y_test_pred_boston)))


sns.set()
plt.figure(figsize=(14,7))
sns.lineplot(y=rmse_train,x=alphas, label='Train RMSE')
sns.lineplot(y=rmse_test,x=alphas, label='Test RMSE')
#plt.xticks(ticks=np.arrange(0.00,0.25,0.01))
#plt.xlim(right=1.4)
#plt.xlim(left=1)
plt.show()

from sklearn.tree import DecisionTreeRegressor
dtr = DecisionTreeRegressor(random_state= 1,ccp_alpha=1.31)
dtr.fit(X_train_boston ,y_train_boston)

from sklearn.metrics import mean_squared_error 
y_pred_dtr = dtr.predict(X_test_boston)
np.sqrt(mean_squared_error(y_test_boston, y_pred_dtr))

y_pred_train_dtr = dtr.predict(X_train_boston)
np.sqrt(mean_squared_error(y_train_boston, y_pred_train_dtr))

df['target'].mean()

plt.figure(figsize=(12,8),dpi=150)
plot_tree(dtr,filled=True,feature_names=X_boston.columns);

dtr.feature_importances_



"""#Ensamble Learning

##Bagging Clasifiers
"""

from sklearn.ensemble import BaggingClassifier
bag_clf = BaggingClassifier(n_estimators=20)

bag_clf.fit(X_train,y_train)

bag_clf_pred = bag_clf.predict(X_test)
bag_clf_pred_train = bag_clf.predict(X_train)

print(accuracy_score(y_test,bag_clf_pred))
print(accuracy_score(y_train,bag_clf_pred_train))

"""## Bagging Regressor"""

from sklearn.ensemble import BaggingRegressor
bag_reg = BaggingRegressor(n_estimators=20)

bag_reg.fit(X_train_boston ,y_train_boston)
y_pred_bagregr = bag_reg.predict(X_test_boston)
np.sqrt(mean_squared_error(y_test_boston,y_pred_bagregr ))

y_pred_bagregr_train = bag_reg.predict(X_train_boston)
np.sqrt(mean_squared_error(y_train_boston,y_pred_bagregr_train))

"""##Bagging Clasifiers with Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
nb_clf = BaggingClassifier(n_estimators=20,base_estimator=GaussianNB())

nb_clf.fit(X_train,y_train)

nb_clf_pred = nb_clf.predict(X_test)
nb_clf_pred_train = nb_clf.predict(X_train)

print(accuracy_score(y_test,nb_clf_pred))
print(accuracy_score(y_train,nb_clf_pred_train))

"""##Voting Classifiers"""

from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier

clf3 = GaussianNB()
clf1 = LogisticRegression()
clf2 =RandomForestClassifier()

eclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)])

eclf1 = eclf1.fit(X_train, y_train)

eclf1_pred = eclf1.predict(X_test)
eclf1_pred_train = eclf1.predict(X_train)

print(accuracy_score(y_test,eclf1_pred))
print(accuracy_score(y_train,eclf1_pred_train))

"""##Random Forest Clasifiers"""

from sklearn.ensemble import RandomForestClassifier
randf = RandomForestClassifier(n_estimators=20,oob_score=True)

randf.fit(X_train,y_train)

randf_pred = randf.predict(X_test)
randf_pred_train = randf.predict(X_train)

print(accuracy_score(y_test,randf_pred))
print(accuracy_score(y_train,randf_pred_train))

randf.oob_score_

randf.feature_importances_

"""##RandomForest Regressor"""

from sklearn.ensemble import RandomForestRegressor
randf_reg = RandomForestRegressor(oob_score=True)

randf_reg.fit(X_train_boston ,y_train_boston)

y_pred_randf_reg = randf_reg.predict(X_test_boston)
np.sqrt(mean_squared_error(y_test_boston, y_pred_randf_reg))

y_pred_train_randf_reg = randf_reg.predict(X_train_boston)
np.sqrt(mean_squared_error(y_train_boston, y_pred_train_randf_reg))

randf_reg.oob_score_

"""##AdaBoost"""

from sklearn.ensemble import AdaBoostClassifier

ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=20)
ada_clf.fit(X_train, y_train)

ada_clf_pred = ada_clf.predict(X_test)
ada_clf_pred_train = ada_clf.predict(X_train)

print(accuracy_score(y_test,ada_clf_pred))
print(accuracy_score(y_train,ada_clf_pred_train))

"""##Adaboost with Logreg base etimator"""

from sklearn.linear_model import LogisticRegression
ada_logreg = AdaBoostClassifier(base_estimator=LogisticRegression(), n_estimators=20)
ada_logreg.fit(X_train, y_train)

ada_logreg_pred = ada_logreg.predict(X_test)
ada_logreg_pred_train = ada_logreg.predict(X_train)

print(accuracy_score(y_test,ada_logreg_pred))
print(accuracy_score(y_train,ada_logreg_pred_train))

"""##Adaboost Regression"""

from sklearn.ensemble import AdaBoostRegressor
ada_reg = AdaBoostRegressor(n_estimators=20)
ada_reg.fit(X_train_boston, y_train_boston)

ada_reg_pred = ada_reg.predict(X_test_boston)
ada_reg_pred_train = ada_reg.predict(X_train_boston)

print(np.sqrt(mean_squared_error(y_test_boston, ada_reg_pred)))
print(np.sqrt(mean_squared_error(y_train_boston, ada_reg_pred_train)))

"""## Gradient Boosting"""

from sklearn.ensemble import GradientBoostingClassifier

gb_clf = GradientBoostingClassifier(n_estimators=20)
gb_clf.fit(X_train, y_train)

gb_pred = gb_clf.predict(X_test)
gb_pred_train = gb_clf.predict(X_train)

print(accuracy_score(y_test,gb_pred))
print(accuracy_score(y_train,gb_pred_train))

"""##Xgboost Clasifiers"""

from xgboost import XGBClassifier

xgb_clf = XGBClassifier()
xgb_clf.fit(X_train, y_train)

xgb_clf_pred = xgb_clf.predict(X_test)
xgb_clf_pred_train = xgb_clf.predict(X_train)

print(accuracy_score(y_test,xgb_clf_pred))
print(accuracy_score(y_train,xgb_clf_pred_train))

"""Param tuning guide :
https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/
"""

xgb_clf = XGBClassifier(gamma=0.25,max_depth=2,learning_rate=0.1,reg_lambda=10)
xgb_clf.fit(X_train, y_train)

xgb_clf_pred = xgb_clf.predict(X_test)
xgb_clf_pred_train = xgb_clf.predict(X_train)

print(accuracy_score(y_test,xgb_clf_pred))
print(accuracy_score(y_train,xgb_clf_pred_train))

"""##Xgboost Regression"""

from xgboost import XGBRegressor

xgbreg = XGBRegressor()
xgbreg.fit(X_train_boston, y_train_boston)

xgbreg_pred = xgbreg.predict(X_test_boston)
xgbreg_pred_train = xgbreg.predict(X_train_boston)

print(np.sqrt(mean_squared_error(y_test_boston, xgbreg_pred)))
print(np.sqrt(mean_squared_error(y_train_boston, xgbreg_pred_train)))

"""Perhaps the most commonly configured hyperparameters are the following:

n_estimators: The number of trees in the ensemble, often increased until no further improvements are seen.

max_depth: The maximum depth of each tree, often values are between 1 and 10.

eta: The learning rate used to weight each model, often set to small values such as 0.3, 0.1, 0.01, or smaller.

subsample: The number of samples (rows) used in each tree, set to a value between 0 and 1, often 1.0 to use all samples.

colsample_bytree: Number of features (columns) used in each tree, set to a value between 0 and 1, often 1.0 to use all features.
"""

xgbmodel = XGBRegressor(gamma=0.25,max_depth=1,learning_rate=0.1,reg_lambda=10,n_estimators=100)

xgbmodel.fit(X_train_boston, y_train_boston,early_stopping_rounds=10,eval_metric='rmse',eval_set=[(X_train_boston, y_train_boston)],verbose=True)

xgbmodel_pred = xgbmodel.predict(X_test_boston)
xgbmodel_pred_train = xgbmodel.predict(X_train_boston)

print(np.sqrt(mean_squared_error(y_test_boston, xgbmodel_pred)))
print(np.sqrt(mean_squared_error(y_train_boston, xgbmodel_pred_train)))

"""## Random Forest Imputer & KNN Imputer"""

!pip install missingpy
!pip install miceforest

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import miceforest as mf
import random
import sklearn.neighbors._base 
import sys
sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base
from missingpy import MissForest
from sklearn.impute import KNNImputer

from sklearn.datasets import fetch_california_housing

df2 = pd.concat(fetch_california_housing(return_X_y=True, as_frame=True),axis=1)
dt2 = df2.copy() #created a copy

df2.head()

df2.isnull().sum()

r1 = random.sample(range(len(df2)),36)
r2 = random.sample(range(len(df2)),34)
r3 = random.sample(range(len(df2)),37)
r4 = random.sample(range(len(df2)),30)
df2['AveRooms'] = [val if i not in r1 else np.nan for i, val in enumerate(dt2['AveRooms'])]
df2['HouseAge'] = [val if i not in r2 else np.nan for i, val in enumerate(dt2['HouseAge'])]
df2['MedHouseVal'] = [val if i not in r3 else np.nan for i, val in enumerate(dt2['MedHouseVal'])]
df2['Latitude'] =    [val if i not in r4 else np.nan for i, val in enumerate(dt2['Latitude'])]

df2.isnull().sum()

"""##Imputation for Random Forest"""

# Create kernels.  #mice forest
kernel = mf.ImputationKernel(
  data=df2,
  save_all_iterations=True,
  random_state=1343
)
# Run the MICE algorithm for 3 iterations on each of the datasets
kernel.mice(3,verbose=True) # error
#print(kernel)
completed_dataset = kernel.complete_data(dataset=1,inplace=False)

imputer = MissForest() #miss forest
X_imputed = imputer.fit_transform(df2)
X_imputed = pd.DataFrame(X_imputed, columns = df2.columns).round(1)

"""##KNN Imputer"""

impute = KNNImputer() #KNN imputation
KNNImputed = impute.fit_transform(df2)
KNNImputed = pd.DataFrame(KNNImputed, columns = df2.columns)

"""## Evaluation"""

missF = np.sum(np.abs(X_imputed[df2.isnull().any(axis=1)] -         dt2[df2.isnull().any(axis=1)]))
mice =  np.sum(np.abs(completed_dataset[df2.isnull().any(axis=1)] - dt2[df2.isnull().any(axis=1)]))
Knn =   np.sum(np.abs(KNNImputed[df2.isnull().any(axis=1)] -        dt2[df2.isnull().any(axis=1)]))
for i in [missF, mice, Knn]:
    print(np.sum(i))

"""From the above outcome, we can have a rough estimate of the efficacy of each method, and it seems Miss Forest was able to recreate the original model better, followed by Mice Forest and then KNN."""

sns.set(style='darkgrid')
fig,ax = plt.subplots(figsize=(35,16),nrows=2, ncols=2,dpi=150)
for col,i,j in zip(['AveRooms','HouseAge', 'Latitude','MedHouseVal'],[0,0,1,1],[0,1,0,1]):
    sns.kdeplot(x = dt2[col][df2.isnull().any(axis=1)], label= 'Original', ax=ax[i][j] )
    sns.kdeplot(x = X_imputed[col][df2.isnull().any(axis=1)], label = 'MissForest', ax=ax[i][j] )
    sns.kdeplot(x = completed_dataset[col][df2.isnull().any(axis=1)], label = 'MiceForest', ax=ax[i][j] )
    sns.kdeplot(x = KNNImputed[col][df2.isnull().any(axis=1)], label='KNNImpute',  ax=ax[i][j] )
    ax[i][j].legend()